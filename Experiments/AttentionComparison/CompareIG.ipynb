{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606c10ad-12b9-440b-b759-83fffbe4352f",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592805a1-c880-4a79-8cba-38b7d341e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "from lime import lime_image\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tf_explain.core.grad_cam import GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b790092-0a4c-4017-8e53-64bbf8bd6457",
   "metadata": {},
   "source": [
    "## Meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ec6a45-7d4a-42a8-a227-cf31ab82a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"..\\\\Models\\\\TrainedModels\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7793a-feaa-423c-8ad2-448630d2a11d",
   "metadata": {},
   "source": [
    "## Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07beae-309f-4bdb-adde-b3237487af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_dir + \"TrainedCbamModel.keras\", compile=True)\n",
    "#model = load_model(model_dir + \"TrainedSeModel.keras\", compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b58ba550-2bbd-4803-813f-96d8f82a3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_MODULE = 'CBAM'\n",
    "#ATTENTION_MODULE = 'SE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626323c-c97e-4522-874c-a4035be30c62",
   "metadata": {},
   "source": [
    "## Preparing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38d94fa-30e2-4e6a-b0b3-d1dedb2ce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\a.jpg\"\n",
    "path_b=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\b.jpg\"\n",
    "path_c=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\c.jpg\"\n",
    "path_d=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\d.jpg\"\n",
    "path_e=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\e.jpg\"\n",
    "path_f=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\f.jpg\"\n",
    "path_g=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\g.jpg\"\n",
    "path_h=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\h.jpg\"\n",
    "path_i=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\i.jpg\"\n",
    "path_j=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\j.jpg\"\n",
    "path_k=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\k.jpg\"\n",
    "path_l=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\l.jpg\"\n",
    "path_m=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\m.jpg\"\n",
    "path_n=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\n.jpg\"\n",
    "path_o=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\o.jpg\"\n",
    "path_p=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\p.jpg\"\n",
    "path_q=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\q.jpg\"\n",
    "path_r=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\r.jpg\"\n",
    "path_s=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\s.jpg\"\n",
    "path_t=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\t.jpg\"\n",
    "path_u=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\u.jpg\"\n",
    "path_v=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\v.jpg\"\n",
    "path_w=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\w.jpg\"\n",
    "path_x=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\x.jpg\"\n",
    "path_y=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\y.jpg\"\n",
    "path_z=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\z.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08d24811-c48b-42fc-acd5-6093ae7d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "image_a = load_img(path_a, target_size=(64, 64))\n",
    "image_array_a = img_to_array(image_a) / 255.0\n",
    "image_input_a = np.expand_dims(image_array_a, axis=0)\n",
    "\n",
    "# b\n",
    "image_b = load_img(path_b, target_size=(64, 64))\n",
    "image_array_b = img_to_array(image_b) / 255.0\n",
    "image_input_b = np.expand_dims(image_array_b, axis=0)\n",
    "\n",
    "# c\n",
    "image_c = load_img(path_c, target_size=(64, 64))\n",
    "image_array_c = img_to_array(image_c) / 255.0\n",
    "image_input_c = np.expand_dims(image_array_c, axis=0)\n",
    "\n",
    "# d\n",
    "image_d = load_img(path_d, target_size=(64, 64))\n",
    "image_array_d = img_to_array(image_d) / 255.0\n",
    "image_input_d = np.expand_dims(image_array_d, axis=0)\n",
    "\n",
    "# e\n",
    "image_e = load_img(path_e, target_size=(64, 64))\n",
    "image_array_e = img_to_array(image_e) / 255.0\n",
    "image_input_e = np.expand_dims(image_array_e, axis=0)\n",
    "\n",
    "# f\n",
    "image_f = load_img(path_f, target_size=(64, 64))\n",
    "image_array_f = img_to_array(image_f) / 255.0\n",
    "image_input_f = np.expand_dims(image_array_f, axis=0)\n",
    "\n",
    "# g\n",
    "image_g = load_img(path_g, target_size=(64, 64))\n",
    "image_array_g = img_to_array(image_g) / 255.0\n",
    "image_input_g = np.expand_dims(image_array_g, axis=0)\n",
    "\n",
    "# h\n",
    "image_h = load_img(path_h, target_size=(64, 64))\n",
    "image_array_h = img_to_array(image_h) / 255.0\n",
    "image_input_h = np.expand_dims(image_array_h, axis=0)\n",
    "\n",
    "# i\n",
    "image_i = load_img(path_i, target_size=(64, 64))\n",
    "image_array_i = img_to_array(image_i) / 255.0\n",
    "image_input_i = np.expand_dims(image_array_i, axis=0)\n",
    "\n",
    "# j\n",
    "image_j = load_img(path_j, target_size=(64, 64))\n",
    "image_array_j = img_to_array(image_j) / 255.0\n",
    "image_input_j = np.expand_dims(image_array_j, axis=0)\n",
    "\n",
    "# k\n",
    "image_k = load_img(path_k, target_size=(64, 64))\n",
    "image_array_k = img_to_array(image_k) / 255.0\n",
    "image_input_k = np.expand_dims(image_array_k, axis=0)\n",
    "\n",
    "# l\n",
    "image_l = load_img(path_l, target_size=(64, 64))\n",
    "image_array_l = img_to_array(image_l) / 255.0\n",
    "image_input_l = np.expand_dims(image_array_l, axis=0)\n",
    "\n",
    "# m\n",
    "image_m = load_img(path_m, target_size=(64, 64))\n",
    "image_array_m = img_to_array(image_m) / 255.0\n",
    "image_input_m = np.expand_dims(image_array_m, axis=0)\n",
    "\n",
    "# n\n",
    "image_n = load_img(path_n, target_size=(64, 64))\n",
    "image_array_n = img_to_array(image_n) / 255.0\n",
    "image_input_n = np.expand_dims(image_array_n, axis=0)\n",
    "\n",
    "# o\n",
    "image_o = load_img(path_o, target_size=(64, 64))\n",
    "image_array_o = img_to_array(image_o) / 255.0\n",
    "image_input_o = np.expand_dims(image_array_o, axis=0)\n",
    "\n",
    "# p\n",
    "image_p = load_img(path_p, target_size=(64, 64))\n",
    "image_array_p = img_to_array(image_p) / 255.0\n",
    "image_input_p = np.expand_dims(image_array_p, axis=0)\n",
    "\n",
    "# q\n",
    "image_q = load_img(path_q, target_size=(64, 64))\n",
    "image_array_q = img_to_array(image_q) / 255.0\n",
    "image_input_q = np.expand_dims(image_array_q, axis=0)\n",
    "\n",
    "# r\n",
    "image_r = load_img(path_r, target_size=(64, 64))\n",
    "image_array_r = img_to_array(image_r) / 255.0\n",
    "image_input_r = np.expand_dims(image_array_r, axis=0)\n",
    "\n",
    "# s\n",
    "image_s = load_img(path_s, target_size=(64, 64))\n",
    "image_array_s = img_to_array(image_s) / 255.0\n",
    "image_input_s = np.expand_dims(image_array_s, axis=0)\n",
    "\n",
    "# t\n",
    "image_t = load_img(path_t, target_size=(64, 64))\n",
    "image_array_t = img_to_array(image_t) / 255.0\n",
    "image_input_t = np.expand_dims(image_array_t, axis=0)\n",
    "\n",
    "# u\n",
    "image_u = load_img(path_u, target_size=(64, 64))\n",
    "image_array_u = img_to_array(image_u) / 255.0\n",
    "image_input_u = np.expand_dims(image_array_u, axis=0)\n",
    "\n",
    "# v\n",
    "image_v = load_img(path_v, target_size=(64, 64))\n",
    "image_array_v = img_to_array(image_v) / 255.0\n",
    "image_input_v = np.expand_dims(image_array_v, axis=0)\n",
    "\n",
    "# w\n",
    "image_w = load_img(path_w, target_size=(64, 64))\n",
    "image_array_w = img_to_array(image_w) / 255.0\n",
    "image_input_w = np.expand_dims(image_array_w, axis=0)\n",
    "\n",
    "# x\n",
    "image_x = load_img(path_x, target_size=(64, 64))\n",
    "image_array_x = img_to_array(image_x) / 255.0\n",
    "image_input_x = np.expand_dims(image_array_x, axis=0)\n",
    "\n",
    "# y\n",
    "image_y = load_img(path_y, target_size=(64, 64))\n",
    "image_array_y = img_to_array(image_y) / 255.0\n",
    "image_input_y = np.expand_dims(image_array_y, axis=0)\n",
    "\n",
    "# z\n",
    "image_z = load_img(path_z, target_size=(64, 64))\n",
    "image_array_z = img_to_array(image_z) / 255.0\n",
    "image_input_z = np.expand_dims(image_array_z, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e0bdb-4337-4090-8314-aba9ac9d18a0",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f0b2074-430a-4376-8a17-d9ed037cb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrated_gradients(model, image, target_class_index, steps=50):    \n",
    "    baseline = np.zeros_like(image)  # All zeros baseline\n",
    "    image_input = tf.convert_to_tensor(image, dtype=tf.float32)  # Convert input image to tensor\n",
    "    \n",
    "      # Interpolate between baseline and image\n",
    "    interpolated_images = [(baseline + (step / steps) * (image - baseline)) for step in range(steps)]\n",
    "    interpolated_images = np.array(interpolated_images)\n",
    "\n",
    "    # Convert the interpolated images to TensorFlow tensors\n",
    "    interpolated_images = tf.convert_to_tensor(interpolated_images, dtype=tf.float32)\n",
    "\n",
    "    # Remove extra dimension to match the input shape expected by the model\n",
    "    interpolated_images = tf.squeeze(interpolated_images, axis=1)  # This removes the extra 1 dimension\n",
    "\n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        predictions = model(interpolated_images)\n",
    "        target_class_probabilities = predictions[:, target_class_index]\n",
    "\n",
    "    grads = tape.gradient(target_class_probabilities, interpolated_images)\n",
    "    integrated_grads = np.mean(grads, axis=0) * (image - baseline)\n",
    "\n",
    "    heatmap = np.sum(integrated_grads, axis=-1)\n",
    "    integrated_gradients_heatmap = np.squeeze(heatmap)\n",
    "\n",
    "    positive_only_heatmap = np.maximum(integrated_gradients_heatmap, 0)\n",
    "\n",
    "    return positive_only_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaf9a6-aa02-410a-879e-2466ca44ef9b",
   "metadata": {},
   "source": [
    "## Retrieve attention maps from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa89d126-cc25-4b4a-8691-307e40485391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_se(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('SE_block' + block_num).attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, 0, 0, :]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e42f46-83e5-4eba-a5bb-0e4044878a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbam_spatial(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('CBAM_block' + block_num).spatial.spatial_attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, :, :, 0]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ddae1-f4c2-46af-8bfa-e8d6edb304e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbam_channel(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('CBAM_block' + block_num).channel.channel_attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, 0, 0, :]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e597d-73c6-4c55-838e-d533bc2f8e4a",
   "metadata": {},
   "source": [
    "## Rescale attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f24bf4c-4a0c-4729-899f-8547741fa5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_up(array, target_size=64):\n",
    "    y = target_size // array.shape[0]\n",
    "    x = target_size // array.shape[1]\n",
    "    \n",
    "    return np.kron(array, np.ones((y, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33940128-f641-4803-a9ec-c6b8ce0ec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense(spatial_attention, technique='avg'):\n",
    "    weight = -1\n",
    "    \n",
    "    if technique == 'avg':\n",
    "        weight = np.mean(spatial_attention)\n",
    "    elif technique == 'max':\n",
    "        weight = np.max(spatial_attention)\n",
    "    elif technique == 'min':\n",
    "        weight = np.min(spatial_attention)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd812ff-a61d-4464-aa37-e72b1a701cb7",
   "metadata": {},
   "source": [
    "## Comparision using kendell corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "46ff9f6d-434e-4bcd-8bdf-9aa76dbaef7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "The IG explanation is 0.0065 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "The IG explanation is -0.0090 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "The IG explanation is 0.0300 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "The IG explanation is 0.0005 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "The IG explanation is 0.0059 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "The IG explanation is -0.0064 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "The IG explanation is -0.0067 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "The IG explanation is 0.0511 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "The IG explanation is -0.0071 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "The IG explanation is 0.0286 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "The IG explanation is 0.0146 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "The IG explanation is 0.0036 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "The IG explanation is -0.0126 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "The IG explanation is -0.0227 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "The IG explanation is -0.0110 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "The IG explanation is -0.0087 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "The IG explanation is 0.0016 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "The IG explanation is 0.0286 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "The IG explanation is 0.0077 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "The IG explanation is -0.0025 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "The IG explanation is 0.0285 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "The IG explanation is 0.0108 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "The IG explanation is -0.0258 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "The IG explanation is -0.0029 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "The IG explanation is 0.0441 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "The IG explanation is -0.0086 similar to the attention weights\n"
     ]
    }
   ],
   "source": [
    "similarities = [] # For each letter\n",
    "attention_level = 0\n",
    "i = 0\n",
    "\n",
    "for letter in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "    # Dynamically retrieve image arrays (assuming they're defined like image_array_A, etc.)\n",
    "    image_array = globals()[f\"image_array_{letter.lower()}\"]\n",
    "    image_input = globals()[f\"image_input_{letter.lower()}\"]\n",
    "\n",
    "    # Getting Integrated Gradients explanation\n",
    "    ig_weight = get_integrated_gradients(model, image_input, i)\n",
    "    \n",
    "    # Retrieving and postprocessing the attention weights\n",
    "    if ATTENTION_MODULE == 'CBAM':\n",
    "        attention_channel = get_cbam_channel(model, image_input, attention_level)\n",
    "        attention_spatial = get_cbam_spatial(model, image_input, attention_level)\n",
    "\n",
    "        channel_condensed = condense(attention_channel)\n",
    "        spatial_scaled = scale_up(attention_spatial)\n",
    "\n",
    "        attention_weights = spatial_scaled * channel_condensed\n",
    "        \n",
    "    elif ATTENTION_MODULE == 'SE':\n",
    "        attention_channel = get_se(model, image_input, attention_level)\n",
    "        channel_condensed = condense(attention_channel)\n",
    "\n",
    "        attention_weights = scale_up(channel_condensed)\n",
    "\n",
    "    # Flatten for correlation\n",
    "    kendal, _ = kendalltau(ig_weight.flatten(), attention_weights.flatten())\n",
    "    similarities.append(kendal)\n",
    "    print(f\"The IG explanation is {kendal:.4f} similar to the attention weights\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7da7a553-8362-4394-94a1-11d8100215f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarity: 0.05114841857881705\n",
      "Minimum of similarity: -0.02579628941800592\n",
      "Mean similarity: 0.005325714207636382\n",
      "Standard deviation of similarity: 0.019110377932915906\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum similarity: {np.max(similarities)}')\n",
    "print(f'Minimum of similarity: {np.min(similarities)}')\n",
    "print(f'Mean similarity: {np.mean(similarities)}')\n",
    "print(f'Standard deviation of similarity: {np.std(similarities)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisualAttentionAsExplanation",
   "language": "python",
   "name": "visualattentionasexplanation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
