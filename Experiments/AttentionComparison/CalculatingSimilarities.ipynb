{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e401b5e-e892-4946-9215-7c55712882ff",
   "metadata": {},
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a697c267-f6e3-4fb2-9900-3b88160aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import math\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "from lime import lime_image\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.stats import kendalltau\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "from FunctionReference import CBAM_Model, SE_Model, Se_functional_model, Cbam_functional_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76958b70-076d-469c-b780-a317f9f78663",
   "metadata": {},
   "source": [
    "## Meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed6a6187-6f1e-4910-ba77-931a54490c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"..\\\\Models\\\\TrainedModels\\\\\"\n",
    "dataset_dir = \"Subset\"\n",
    "\n",
    "saving_treshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501d193-5ed1-4e67-ae4e-9ab9737a941a",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71af22cd-2cad-43fc-9292-e151edd5e42a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=..\\Models\\TrainedModels\\TrainedCbamModel.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cbam_model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrainedCbamModel.keras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m se_model = load_model(model_dir + \u001b[33m\"\u001b[39m\u001b[33mTrainedSeModel.keras\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\VisualAttentionAsExplanation\\notebooks\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:203\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format.load_model_from_hdf5(\n\u001b[32m    197\u001b[39m         filepath,\n\u001b[32m    198\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    199\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    200\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    201\u001b[39m     )\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     )\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    210\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmight have a different name).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=..\\Models\\TrainedModels\\TrainedCbamModel.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "cbam_model = load_model(model_dir + \"TrainedCbamModel.keras\", compile=True)\n",
    "se_model = load_model(model_dir + \"TrainedSeModel.keras\", compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e6e02-235c-4620-ae29-6a20fe6edca2",
   "metadata": {},
   "source": [
    "## Creating functional model and copying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6d4be-2749-4209-a74c-05c677647629",
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_cbam_model = cbam_functional_model(LABEL_CLASS, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "functional_se_model = Se_functional_model(LABEL_CLASS, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652c534-62d7-4423-997f-d5e7f3d3aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in functional_cbam_model.layers:\n",
    "    try:\n",
    "        layer.set_weights(cbam_model.get_layer(layer.name).get_weights())\n",
    "    except (ValueError, AttributeError):\n",
    "        print(layer.name)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465670bd-ff27-4d05-a67d-759f66230135",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in functional_se_model.layers:\n",
    "    try:\n",
    "        layer.set_weights(se_model.get_layer(layer.name).get_weights())\n",
    "    except (ValueError, AttributeError):\n",
    "        print(layer.name)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62982d-77c2-46d3-830d-e77341d61a5a",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b980192b-a050-413b-9c90-536758f7adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories_images(path):\n",
    "    images = []\n",
    "\n",
    "    for Image in os.listdir(path + \"\\\\\"):\n",
    "        images.append(path + \"\\\\\" + Image)\n",
    "\n",
    "    return pd.DataFrame({'images': images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ebd06d4-0946-446a-b67b-29d67d86c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_directories_images(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1593768-5835-4c98-b778-c6fab88dc35d",
   "metadata": {},
   "source": [
    "## Explanation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cd177-483a-47b7-9c8d-a94ed021df9b",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65c95de5-9973-4c0c-bbba-25fb0288ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradcam(model, image, layer_name):      \n",
    "    # Grad-CAM\n",
    "    explainer = GradCAM()\n",
    "    predictions = model.predict(image)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "    # Apply Grad-CAM to obtain the heatmap\n",
    "    cams = explainer.explain(\n",
    "        validation_data=(image, None),\n",
    "        model=model,\n",
    "        layer_name=layer_name,\n",
    "        class_index=predicted_class\n",
    "    )\n",
    "\n",
    "    # Post processing of the heatmap\n",
    "    heatmap = cams[0].numpy()\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    if heatmap.shape != (height, width):\n",
    "        heatmap = cv2.resize(heatmap, (width, height))\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18729f7-c038-4529-8b99-d6cadfd5f4c1",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a59a6241-dcc4-404f-86ec-94fcb22639bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lime(model, image_array_u, image_input_u):\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "    # Explain the instance\n",
    "    explanation = explainer.explain_instance(\n",
    "        image_array_u,         \n",
    "        model.predict,\n",
    "        top_labels=26,\n",
    "        hide_color=0,\n",
    "        num_samples=1000\n",
    "    )\n",
    "\n",
    "    # Get explanation for the top predicted class\n",
    "    label = explanation.top_labels[0]\n",
    "    dict_heatmap = dict(explanation.local_exp[label])\n",
    "    segments = explanation.segments\n",
    "\n",
    "    # Create pixel-wise heatmap from superpixel weights\n",
    "    heatmap = np.zeros(segments.shape)\n",
    "    for segment_id, weight in dict_heatmap.items():\n",
    "        heatmap[segments == segment_id] = weight\n",
    "\n",
    "    only_positive_heatmap = np.maximum(heatmap, 0)\n",
    "    \n",
    "    return only_positive_heatmap "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540e6ad-004b-4b77-a198-dc26e2cea0ad",
   "metadata": {},
   "source": [
    "### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecd9c7e8-fbd6-4452-918d-6b650cfa8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrated_gradients(model, image, target_class_index, steps=50):    \n",
    "    baseline = np.zeros_like(image)  # All zeros baseline\n",
    "    image_input = tf.convert_to_tensor(image, dtype=tf.float32)  # Convert input image to tensor\n",
    "    \n",
    "      # Interpolate between baseline and image\n",
    "    interpolated_images = [(baseline + (step / steps) * (image - baseline)) for step in range(steps)]\n",
    "    interpolated_images = np.array(interpolated_images)\n",
    "\n",
    "    # Convert the interpolated images to TensorFlow tensors\n",
    "    interpolated_images = tf.convert_to_tensor(interpolated_images, dtype=tf.float32)\n",
    "\n",
    "    # Remove extra dimension to match the input shape expected by the model\n",
    "    interpolated_images = tf.squeeze(interpolated_images, axis=1)  # This removes the extra 1 dimension\n",
    "\n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        predictions = model(interpolated_images)\n",
    "        target_class_probabilities = predictions[:, target_class_index]\n",
    "\n",
    "    grads = tape.gradient(target_class_probabilities, interpolated_images)\n",
    "    integrated_grads = np.mean(grads, axis=0) * (image - baseline)\n",
    "\n",
    "    heatmap = np.sum(integrated_grads, axis=-1)\n",
    "    integrated_gradients_heatmap = np.squeeze(heatmap)\n",
    "\n",
    "    positive_only_heatmap = np.maximum(integrated_gradients_heatmap, 0)\n",
    "\n",
    "    return positive_only_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3a087-07b2-4b9e-8eb7-fe1641f5d66c",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50ef6819-6780-45d1-bc66-71c2b9b1c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_up(array, target_size=64):\n",
    "    y = target_size // array.shape[0]\n",
    "    x = target_size // array.shape[1]\n",
    "    \n",
    "    return np.kron(array, np.ones((y, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eed6710c-c0aa-42f8-ae46-35108b05d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense(spatial_attention, technique='avg'):\n",
    "    weight = -1\n",
    "    \n",
    "    if technique == 'avg':\n",
    "        weight = np.mean(spatial_attention)\n",
    "    elif technique == 'max':\n",
    "        weight = np.max(spatial_attention)\n",
    "    elif technique == 'min':\n",
    "        weight = np.min(spatial_attention)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0bd58-fa7b-44c6-bcb0-9130dcd2fa2c",
   "metadata": {},
   "source": [
    "## Calculating similarities using multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2e65d-c745-46bf-9036-b3559d2357ba",
   "metadata": {},
   "source": [
    "### Wrapper for running functions concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab59fe93-a528-451a-bc55-7c8c9257e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_attention(model, image):\n",
    "    _ = model(image, save_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "732cfb28-f031-4172-b32f-b90c4439ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_return(target_func, return_dict, key, *args):\n",
    "    result = target_func(*args)\n",
    "    return_dict[key] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8c21b-38f3-431a-b553-b8b7710bc61a",
   "metadata": {},
   "source": [
    "### Performing calculations, continuosly saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6a83e64-a411-46f1-b389-06eb4e1f1339",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'functional_cbam_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m cbam_explanations = {}\n\u001b[32m     28\u001b[39m se_explanations = {}\n\u001b[32m     30\u001b[39m cbam_threads = [\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam0\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mfunctional_cbam_model\u001b[49m, image_input, \u001b[32m0\u001b[39m)),\n\u001b[32m     32\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam1\u001b[39m\u001b[33m\"\u001b[39m, functional_cbam_model, image_input, \u001b[32m1\u001b[39m)),\n\u001b[32m     33\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam2\u001b[39m\u001b[33m\"\u001b[39m, functional_cbam_model, image_input, \u001b[32m2\u001b[39m)),\n\u001b[32m     34\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam3\u001b[39m\u001b[33m\"\u001b[39m, functional_cbam_model, image_input, \u001b[32m3\u001b[39m)),\n\u001b[32m     35\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam4\u001b[39m\u001b[33m\"\u001b[39m, functional_cbam_model, image_input, \u001b[32m4\u001b[39m)),\n\u001b[32m     36\u001b[39m     threading.Thread(target=run_with_return, args=(get_lime, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mlime\u001b[39m\u001b[33m\"\u001b[39m, cbam_model, image_array, image_input)),\n\u001b[32m     37\u001b[39m     threading.Thread(target=run_with_return, args=(get_integrated_gradients, cbam_explanations, \u001b[33m\"\u001b[39m\u001b[33mig\u001b[39m\u001b[33m\"\u001b[39m, cbam_model, image_input, target_class_index)),\n\u001b[32m     38\u001b[39m     threading.Thread(target=save_attention, args=(cbam_model, image_input))\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     41\u001b[39m se_threads = [\n\u001b[32m     42\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam0\u001b[39m\u001b[33m\"\u001b[39m, functional_se_model, image_input, \u001b[32m0\u001b[39m)),\n\u001b[32m     43\u001b[39m     threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \u001b[33m\"\u001b[39m\u001b[33mgradcam1\u001b[39m\u001b[33m\"\u001b[39m, functional_se_model, image_input, \u001b[32m1\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     threading.Thread(target=save_attention, args=(se_model, image_input))\n\u001b[32m     50\u001b[39m ]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m cbam_threads + se_threads:\n",
      "\u001b[31mNameError\u001b[39m: name 'functional_cbam_model' is not defined"
     ]
    }
   ],
   "source": [
    "images_processed = 0\n",
    "images_treshold = math.ceil(len(df) / saving_treshold)\n",
    "\n",
    "explanation_names = ['gradcam', 'lime', 'ig']\n",
    "\n",
    "model_names = []\n",
    "attention_types = []\n",
    "layer_nums = []\n",
    "explanation_types = []\n",
    "scores = []\n",
    "\n",
    "model_names1 = []\n",
    "gradcam0_lime_scores = []\n",
    "gradcam1_lime_scores = []\n",
    "gradcam2_lime_scores = []\n",
    "gradcam3_lime_scores = []\n",
    "gradcam4_lime_scores = []\n",
    "gradcam0_ig_scores = []\n",
    "gradcam1_ig_scores = []\n",
    "gradcam2_ig_scores = []\n",
    "gradcam3_ig_scores = []\n",
    "gradcam4_ig_scores = []\n",
    "lime_ig_scores = []\n",
    "\n",
    "for index, image_dir in df.iterrows():\n",
    "    image = load_img(image_dir.iloc[0], target_size=(64, 64))\n",
    "    image_array = img_to_array(image) / 255.0\n",
    "    image_input = np.expand_dims(image, axis=0)\n",
    "\n",
    "    cbam_explanations = {}\n",
    "    se_explanations = {}\n",
    "\n",
    "    cbam_threads = [\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \"gradcam0\", functional_cbam_model, image_input, 0)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \"gradcam1\", functional_cbam_model, image_input, 1)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \"gradcam2\", functional_cbam_model, image_input, 2)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \"gradcam3\", functional_cbam_model, image_input, 3)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, cbam_explanations, \"gradcam4\", functional_cbam_model, image_input, 4)),\n",
    "        threading.Thread(target=run_with_return, args=(get_lime, cbam_explanations, \"lime\", cbam_model, image_array, image_input)),\n",
    "        threading.Thread(target=run_with_return, args=(get_integrated_gradients, cbam_explanations, \"ig\", cbam_model, image_input, target_class_index)),\n",
    "        threading.Thread(target=save_attention, args=(cbam_model, image_input))\n",
    "    ]\n",
    "\n",
    "    se_threads = [\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \"gradcam0\", functional_se_model, image_input, 0)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \"gradcam1\", functional_se_model, image_input, 1)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \"gradcam2\", functional_se_model, image_input, 2)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \"gradcam3\", functional_se_model, image_input, 3)),\n",
    "        threading.Thread(target=run_with_return, args=(get_gradcam, se_explanations, \"gradcam4\", functional_se_model, image_input, 4)),\n",
    "        threading.Thread(target=run_with_return, args=(get_lime, se_explanations, \"lime\", se_model, image_array, image_input)),\n",
    "        threading.Thread(target=run_with_return, args=(get_integrated_gradients, se_explanations, \"ig\", se_model, image_input, target_class_index)),\n",
    "        threading.Thread(target=save_attention, args=(se_model, image_input))\n",
    "    ]\n",
    "\n",
    "    for thread in cbam_threads + se_threads:\n",
    "        t.start()\n",
    "\n",
    "    for thread in cbam_threads + se_threads:\n",
    "        t.join()\n",
    "\n",
    "    # Calculating the similarities between different explanation techniques\n",
    "    se_gradcam0_lime_score, _ = kendalltau(se_explanations['gradcam0'].flatten(), se_explanations['lime'].flatten())\n",
    "    se_gradcam1_lime_score, _ = kendalltau(se_explanations['gradcam1'].flatten(), se_explanations['lime'].flatten())\n",
    "    se_gradcam2_lime_score, _ = kendalltau(se_explanations['gradcam2'].flatten(), se_explanations['lime'].flatten())\n",
    "    se_gradcam3_lime_score, _ = kendalltau(se_explanations['gradcam3'].flatten(), se_explanations['lime'].flatten())\n",
    "    se_gradcam4_lime_score, _ = kendalltau(se_explanations['gradcam4'].flatten(), se_explanations['lime'].flatten())\n",
    "    se_gradcam0_ig_score, _ = kendalltau(se_explanations['gradcam0'].flatten(), se_explanations['ig'].flatten())\n",
    "    se_gradcam1_ig_score, _ = kendalltau(se_explanations['gradcam1'].flatten(), se_explanations['ig'].flatten())\n",
    "    se_gradcam2_ig_score, _ = kendalltau(se_explanations['gradcam2'].flatten(), se_explanations['ig'].flatten())\n",
    "    se_gradcam3_ig_score, _ = kendalltau(se_explanations['gradcam3'].flatten(), se_explanations['ig'].flatten())\n",
    "    se_gradcam4_ig_score, _ = kendalltau(se_explanations['gradcam4'].flatten(), se_explanations['ig'].flatten())\n",
    "    se_lime_ig_score, _ = kendalltau(se_explanations['lime'].flatten(), se_explanations['ig'].flatten())\n",
    "\n",
    "    \n",
    "    model_names1.append('SE')\n",
    "    gradcam0_lime_scores.append(se_gradcam0_lime_score)\n",
    "    gradcam1_lime_scores.append(se_gradcam1_lime_score)\n",
    "    gradcam2_lime_scores.append(se_gradcam2_lime_score)\n",
    "    gradcam3_lime_scores.append(se_gradcam3_lime_score)\n",
    "    gradcam4_lime_scores.append(se_gradcam4_lime_score)\n",
    "    gradcam0_ig_scores.append(se_gradcam0_ig_score)\n",
    "    gradcam1_ig_scores.append(se_gradcam1_ig_score)\n",
    "    gradcam2_ig_scores.append(se_gradcam2_ig_score)\n",
    "    gradcam3_ig_scores.append(se_gradcam3_ig_score)\n",
    "    gradcam4_ig_scores.append(se_gradcam4_ig_score)\n",
    "    lime_ig_scores.append(se_lime_ig_score)\n",
    "\n",
    "    cbam_gradcam0_lime_score, _ = kendalltau(se_explanations['gradcam0'].flatten(), se_explanations['lime'].flatten())\n",
    "    cbam_gradcam1_lime_score, _ = kendalltau(se_explanations['gradcam1'].flatten(), se_explanations['lime'].flatten())\n",
    "    cbam_gradcam2_lime_score, _ = kendalltau(se_explanations['gradcam2'].flatten(), se_explanations['lime'].flatten())\n",
    "    cbam_gradcam3_lime_score, _ = kendalltau(se_explanations['gradcam3'].flatten(), se_explanations['lime'].flatten())\n",
    "    cbam_gradcam4_lime_score, _ = kendalltau(se_explanations['gradcam4'].flatten(), se_explanations['lime'].flatten())\n",
    "    cbam_gradcam0_ig_score, _ = kendalltau(se_explanations['gradcam0'].flatten(), se_explanations['ig'].flatten())\n",
    "    cbam_gradcam1_ig_score, _ = kendalltau(se_explanations['gradcam1'].flatten(), se_explanations['ig'].flatten())\n",
    "    cbam_gradcam2_ig_score, _ = kendalltau(se_explanations['gradcam2'].flatten(), se_explanations['ig'].flatten())\n",
    "    cbam_gradcam3_ig_score, _ = kendalltau(se_explanations['gradcam3'].flatten(), se_explanations['ig'].flatten())\n",
    "    cbam_gradcam4_ig_score, _ = kendalltau(se_explanations['gradcam4'].flatten(), se_explanations['ig'].flatten())\n",
    "    cbam_lime_ig_score, _ = kendalltau(cbam_explanations['lime'].flatten(), cbam_explanations['ig'].flatten())\n",
    "\n",
    "    model_names1.append('CBAM')\n",
    "    gradcam0_lime_scores.append(cbam_gradcam0_lime_score)\n",
    "    gradcam1_lime_scores.append(cbam_gradcam1_lime_score)\n",
    "    gradcam2_lime_scores.append(cbam_gradcam2_lime_score)\n",
    "    gradcam3_lime_scores.append(cbam_gradcam3_lime_score)\n",
    "    gradcam4_lime_scores.append(cbam_gradcam4_lime_score)\n",
    "    gradcam0_ig_scores.append(cbam_gradcam0_ig_score)\n",
    "    gradcam1_ig_scores.append(cbam_gradcam1_ig_score)\n",
    "    gradcam2_ig_scores.append(cbam_gradcam2_ig_score)\n",
    "    gradcam3_ig_scores.append(cbam_gradcam3_ig_score)\n",
    "    gradcam4_ig_scores.append(cbam_gradcam4_ig_score)\n",
    "    lime_ig_scores.append(cbam_lime_ig_score)\n",
    "    \n",
    "    # Calculating the similarities between explanation techniques and attention weights\n",
    "    for layer_num in range(0, 5):\n",
    "        for explanation_name in explanation_names:\n",
    "            if explanation_name == 'gradcam':\n",
    "                explanation_name = explanation_name + str(layer_num)\n",
    "            \n",
    "            # SE\n",
    "            se_explanation = se_explanations[explanation_name]\n",
    "            \n",
    "            # Channel attention\n",
    "            se_attention_tensor = se_model.get_layer('SE_block' + layer_num).attention_map\n",
    "            se_attention_resized = se_attention_tensor[0, 0, 0, :]\n",
    "            se_attention_array = se_attention_resized.numpy()\n",
    "            se_attention_condensed = condense(se_attention_array)\n",
    "            se_attention_weights = scale_up(se_attention_condensed)\n",
    "            \n",
    "            se_channel_score, _ = kendalltau(se_explanation.flatten(), se_attention_weights.flatten())\n",
    "\n",
    "            # CBAM\n",
    "            cbam_explanation = cbam_explanations[explanation_name]\n",
    "            \n",
    "            # Channel attention\n",
    "            cbam_channel_attention_tensor = cbam_model.get_layer('CBAM_block' + layer_num).channel.channel_attention_map\n",
    "            cbam_channel_attention_resized = cbam_channel_attention_tensor[0, 0, 0, :]\n",
    "            cbam_channel_attention_array = cbam_channel_attention_resized.numpy()\n",
    "            cbam_channel_attention_condensed = condense(cbam_channel_attention_array)\n",
    "            cbam_channel_attention_weights = scale_up(cbam_channel_attention_condensed)\n",
    "\n",
    "            cbam_channel_score, _ = kendalltau(cbam_explanation.flatten(), cbam_channel_attention_weights.flatten())\n",
    "\n",
    "            # Convolutional Block Attention Module spatial attention\n",
    "            cbam_spatial_attention_tensor = cbam_model.get_layer('CBAM_block' + layer_num).spatial.spatial_attention_map\n",
    "            cbam_spatial_attention_array = cbam_spatial_attention_tensor.numpy()\n",
    "            cbam_spatial_attention_weights = scale_up(cbam_spatial_attention_array)\n",
    "\n",
    "            cbam_spatial_score, _ = kendalltau(cbam_explanation.flatten(), cbam_spatial_attention_weights.flatten())\n",
    "    \n",
    "            # Convolutional Block Attention Module channel-spatial attention\n",
    "            cbam_spatial_channel_attention_array = cbam_channel_attention_condensed * cbam_spatial_attention_array\n",
    "            cbam_spatial_channel_attention_weights = scale_up(cbam_spatial_channel_attention_array)\n",
    "\n",
    "            cbam_spatial_channel_score, _ = kendalltau(cbam_explanation.flatten(), cbam_spatial_channel_attention_weights.flatten())\n",
    "            \n",
    "            # Storing results\n",
    "            attention_types.append('channel')\n",
    "            explanation_types.append(explanation_name)\n",
    "            layer_nums.append(layer_num)\n",
    "            model_names.append('SE')\n",
    "            scores.append(se_channel_score)\n",
    "\n",
    "            attention_types.append('channel')\n",
    "            explanation_types.append(explanation_name)\n",
    "            layer_nums.append(layer_num)\n",
    "            model_names.append('CBAM')\n",
    "            scores.append(cbam_channel_score)\n",
    "\n",
    "            attention_types.append('spatial')\n",
    "            explanation_types.append(explanation_name)\n",
    "            layer_nums.append(layer_num)\n",
    "            model_names.append('CBAM')\n",
    "            scores.append(cbam_spatial_score)\n",
    "\n",
    "            attention_types.append('channel_spatial')\n",
    "            explanation_types.append(explanation_name)\n",
    "            layer_nums.append(layer_num)\n",
    "            model_names.append('CBAM')\n",
    "            scores.append(cbam_spatial_channel_score)\n",
    "\n",
    "    image_processed += 1\n",
    "\n",
    "    # Saving data to file if enough images was processed\n",
    "    if image_processed >= image_treshold:\n",
    "        attention_dataframe = {'model': model_names, 'attention_type': attention_types, 'layer': layer_nums, 'explanation': explanation_types, 'score': scores}\n",
    "        explanation_dataframe = {'model': model_names1, 'gradcam_lime': gradcam_lime_scores, 'gradcam_ig': gradcam_ig_scores, 'lime_ig': lime_ig_scores}\n",
    "\n",
    "        attention_dataframe.to_hdf(\"similarity_scores.h5\", key=\"attention\", mode=\"a\")\n",
    "        explanation_dataframe.to_hdf(\"similarity_scores.h5\", key=\"explanation\", mode=\"a\")\n",
    "\n",
    "        model_names.clear()\n",
    "        attention_types.clear()\n",
    "        layer_nums.clear()\n",
    "        explanation_types.clear()\n",
    "        scores.clear()\n",
    "        \n",
    "        model_names1.clear()\n",
    "        gradcam_lime_scores.clear()\n",
    "        gradcam_ig_scores.clear()\n",
    "        lime_ig_scores.clear()\n",
    "\n",
    "        model_names1.clear()\n",
    "        gradcam0_lime_scores.clear()\n",
    "        gradcam1_lime_scores.clear()\n",
    "        gradcam2_lime_scores.clear()\n",
    "        gradcam3_lime_scores.clear()\n",
    "        gradcam4_lime_scores.clear()\n",
    "        gradcam0_ig_scores.clear()\n",
    "        gradcam1_ig_scores.clear()\n",
    "        gradcam2_ig_scores.clear()\n",
    "        gradcam3_ig_scores.clear()\n",
    "        gradcam4_ig_scores.clear()\n",
    "        lime_ig_scores.clear()\n",
    "        \n",
    "        image_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e985681-1781-4cbf-badb-67a8e0fb1788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisualAttentionAsExplanation",
   "language": "python",
   "name": "visualattentionasexplanation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
