{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606c10ad-12b9-440b-b759-83fffbe4352f",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592805a1-c880-4a79-8cba-38b7d341e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "from lime import lime_image\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from GradCAM import GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b790092-0a4c-4017-8e53-64bbf8bd6457",
   "metadata": {},
   "source": [
    "## Meta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c83aa64-1e96-4494-8491-e1c6e7d6d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CLASS = 26\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_CHANNELS= 3\n",
    "\n",
    "saving_dir = \"CustomDatasetGradCAM\\\\\"\n",
    "model_dir = \"..\\\\Models\\\\TrainedModels\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7793a-feaa-423c-8ad2-448630d2a11d",
   "metadata": {},
   "source": [
    "## Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eb738f8-4ecc-49af-b9c5-409a5a245521",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mload_model\u001b[49m(model_dir + \u001b[33m\"\u001b[39m\u001b[33mTrainedCbamModel.keras\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#model = load_model(model_dir + \"TrainedSeModel.keras\", compile=True)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model(model_dir + \"TrainedCbamModel.keras\", compile=True)\n",
    "#model = load_model(model_dir + \"TrainedSeModel.keras\", compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b58ba550-2bbd-4803-813f-96d8f82a3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_MODULE = 'CBAM'\n",
    "#ATTENTION_MODULE = 'SE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d04cf0-c0e2-4f97-9b3a-76542ef40a5a",
   "metadata": {},
   "source": [
    "## Creating functional model and copying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e752c2-b152-48d0-9f54-a5693bb71f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_model = Cbam_functional_model(LABEL_CLASS, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "#functional_model = Se_functional_model(LABEL_CLASS, IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "\n",
    "functional_model.build((None, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118b27c-f988-4265-b7ac-005b21e25b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in functional_model.layers:\n",
    "    try:\n",
    "        layer.set_weights(model.get_layer(layer.name).get_weights())\n",
    "    except (ValueError, AttributeError):\n",
    "        print(layer.name)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626323c-c97e-4522-874c-a4035be30c62",
   "metadata": {},
   "source": [
    "## Preparing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e38d94fa-30e2-4e6a-b0b3-d1dedb2ce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\a.jpg\"\n",
    "path_b=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\b.jpg\"\n",
    "path_c=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\c.jpg\"\n",
    "path_d=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\d.jpg\"\n",
    "path_e=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\e.jpg\"\n",
    "path_f=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\f.jpg\"\n",
    "path_g=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\g.jpg\"\n",
    "path_h=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\h.jpg\"\n",
    "path_i=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\i.jpg\"\n",
    "path_j=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\j.jpg\"\n",
    "path_k=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\k.jpg\"\n",
    "path_l=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\l.jpg\"\n",
    "path_m=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\m.jpg\"\n",
    "path_n=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\n.jpg\"\n",
    "path_o=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\o.jpg\"\n",
    "path_p=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\p.jpg\"\n",
    "path_q=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\q.jpg\"\n",
    "path_r=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\r.jpg\"\n",
    "path_s=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\s.jpg\"\n",
    "path_t=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\t.jpg\"\n",
    "path_u=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\u.jpg\"\n",
    "path_v=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\v.jpg\"\n",
    "path_w=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\w.jpg\"\n",
    "path_x=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\x.jpg\"\n",
    "path_y=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\y.jpg\"\n",
    "path_z=\"..\\\\Datasets\\\\AdditionalDatasets\\\\FatimaDataset\\\\z.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08d24811-c48b-42fc-acd5-6093ae7d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "image_a = load_img(path_a, target_size=(64, 64))\n",
    "image_array_a = img_to_array(image_a) / 255.0\n",
    "image_input_a = np.expand_dims(image_array_a, axis=0)\n",
    "\n",
    "# b\n",
    "image_b = load_img(path_b, target_size=(64, 64))\n",
    "image_array_b = img_to_array(image_b) / 255.0\n",
    "image_input_b = np.expand_dims(image_array_b, axis=0)\n",
    "\n",
    "# c\n",
    "image_c = load_img(path_c, target_size=(64, 64))\n",
    "image_array_c = img_to_array(image_c) / 255.0\n",
    "image_input_c = np.expand_dims(image_array_c, axis=0)\n",
    "\n",
    "# d\n",
    "image_d = load_img(path_d, target_size=(64, 64))\n",
    "image_array_d = img_to_array(image_d) / 255.0\n",
    "image_input_d = np.expand_dims(image_array_d, axis=0)\n",
    "\n",
    "# e\n",
    "image_e = load_img(path_e, target_size=(64, 64))\n",
    "image_array_e = img_to_array(image_e) / 255.0\n",
    "image_input_e = np.expand_dims(image_array_e, axis=0)\n",
    "\n",
    "# f\n",
    "image_f = load_img(path_f, target_size=(64, 64))\n",
    "image_array_f = img_to_array(image_f) / 255.0\n",
    "image_input_f = np.expand_dims(image_array_f, axis=0)\n",
    "\n",
    "# g\n",
    "image_g = load_img(path_g, target_size=(64, 64))\n",
    "image_array_g = img_to_array(image_g) / 255.0\n",
    "image_input_g = np.expand_dims(image_array_g, axis=0)\n",
    "\n",
    "# h\n",
    "image_h = load_img(path_h, target_size=(64, 64))\n",
    "image_array_h = img_to_array(image_h) / 255.0\n",
    "image_input_h = np.expand_dims(image_array_h, axis=0)\n",
    "\n",
    "# i\n",
    "image_i = load_img(path_i, target_size=(64, 64))\n",
    "image_array_i = img_to_array(image_i) / 255.0\n",
    "image_input_i = np.expand_dims(image_array_i, axis=0)\n",
    "\n",
    "# j\n",
    "image_j = load_img(path_j, target_size=(64, 64))\n",
    "image_array_j = img_to_array(image_j) / 255.0\n",
    "image_input_j = np.expand_dims(image_array_j, axis=0)\n",
    "\n",
    "# k\n",
    "image_k = load_img(path_k, target_size=(64, 64))\n",
    "image_array_k = img_to_array(image_k) / 255.0\n",
    "image_input_k = np.expand_dims(image_array_k, axis=0)\n",
    "\n",
    "# l\n",
    "image_l = load_img(path_l, target_size=(64, 64))\n",
    "image_array_l = img_to_array(image_l) / 255.0\n",
    "image_input_l = np.expand_dims(image_array_l, axis=0)\n",
    "\n",
    "# m\n",
    "image_m = load_img(path_m, target_size=(64, 64))\n",
    "image_array_m = img_to_array(image_m) / 255.0\n",
    "image_input_m = np.expand_dims(image_array_m, axis=0)\n",
    "\n",
    "# n\n",
    "image_n = load_img(path_n, target_size=(64, 64))\n",
    "image_array_n = img_to_array(image_n) / 255.0\n",
    "image_input_n = np.expand_dims(image_array_n, axis=0)\n",
    "\n",
    "# o\n",
    "image_o = load_img(path_o, target_size=(64, 64))\n",
    "image_array_o = img_to_array(image_o) / 255.0\n",
    "image_input_o = np.expand_dims(image_array_o, axis=0)\n",
    "\n",
    "# p\n",
    "image_p = load_img(path_p, target_size=(64, 64))\n",
    "image_array_p = img_to_array(image_p) / 255.0\n",
    "image_input_p = np.expand_dims(image_array_p, axis=0)\n",
    "\n",
    "# q\n",
    "image_q = load_img(path_q, target_size=(64, 64))\n",
    "image_array_q = img_to_array(image_q) / 255.0\n",
    "image_input_q = np.expand_dims(image_array_q, axis=0)\n",
    "\n",
    "# r\n",
    "image_r = load_img(path_r, target_size=(64, 64))\n",
    "image_array_r = img_to_array(image_r) / 255.0\n",
    "image_input_r = np.expand_dims(image_array_r, axis=0)\n",
    "\n",
    "# s\n",
    "image_s = load_img(path_s, target_size=(64, 64))\n",
    "image_array_s = img_to_array(image_s) / 255.0\n",
    "image_input_s = np.expand_dims(image_array_s, axis=0)\n",
    "\n",
    "# t\n",
    "image_t = load_img(path_t, target_size=(64, 64))\n",
    "image_array_t = img_to_array(image_t) / 255.0\n",
    "image_input_t = np.expand_dims(image_array_t, axis=0)\n",
    "\n",
    "# u\n",
    "image_u = load_img(path_u, target_size=(64, 64))\n",
    "image_array_u = img_to_array(image_u) / 255.0\n",
    "image_input_u = np.expand_dims(image_array_u, axis=0)\n",
    "\n",
    "# v\n",
    "image_v = load_img(path_v, target_size=(64, 64))\n",
    "image_array_v = img_to_array(image_v) / 255.0\n",
    "image_input_v = np.expand_dims(image_array_v, axis=0)\n",
    "\n",
    "# w\n",
    "image_w = load_img(path_w, target_size=(64, 64))\n",
    "image_array_w = img_to_array(image_w) / 255.0\n",
    "image_input_w = np.expand_dims(image_array_w, axis=0)\n",
    "\n",
    "# x\n",
    "image_x = load_img(path_x, target_size=(64, 64))\n",
    "image_array_x = img_to_array(image_x) / 255.0\n",
    "image_input_x = np.expand_dims(image_array_x, axis=0)\n",
    "\n",
    "# y\n",
    "image_y = load_img(path_y, target_size=(64, 64))\n",
    "image_array_y = img_to_array(image_y) / 255.0\n",
    "image_input_y = np.expand_dims(image_array_y, axis=0)\n",
    "\n",
    "# z\n",
    "image_z = load_img(path_z, target_size=(64, 64))\n",
    "image_array_z = img_to_array(image_z) / 255.0\n",
    "image_input_z = np.expand_dims(image_array_z, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e0bdb-4337-4090-8314-aba9ac9d18a0",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0b2074-430a-4376-8a17-d9ed037cb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradcam(model, image, layer_name):      \n",
    "    # Grad-CAM\n",
    "    explainer = GradCAM()\n",
    "    predictions = model.predict(image)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "    # Apply Grad-CAM to obtain the heatmap\n",
    "    cams = explainer.explain(\n",
    "        validation_data=(image, None),\n",
    "        model=model,\n",
    "        layer_name=layer_name,\n",
    "        class_index=predicted_class\n",
    "    )\n",
    "\n",
    "    # Post processing of the heatmap\n",
    "    heatmap = cams[0].numpy()\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    if heatmap.shape != (height, width):\n",
    "        heatmap = cv2.resize(heatmap, (width, height))\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaff94-daba-4963-b8c5-78fd8fddd254",
   "metadata": {},
   "source": [
    "## Choose layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fda7f19c-470b-43b0-9385-72d561d5e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'block5_conv4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaf9a6-aa02-410a-879e-2466ca44ef9b",
   "metadata": {},
   "source": [
    "## Retrieve attention maps from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b32aa9-c6b3-4281-87ac-3312273fa84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_se(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('SE_block' + block_num).attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, 0, 0, :]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20285de8-f3b8-40df-89db-ae04708cbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbam_spatial(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('CBAM_block' + block_num).spatial.spatial_attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, :, :, 0]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622683c6-9d05-4080-8906-d7c247c2a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cbam_channel(model, image, block_num):\n",
    "    # Making the prediction and saving the attention weights\n",
    "    _ = model(image, save_attention=True)\n",
    "\n",
    "    # Retrieving the tensor from the model\n",
    "    attention_tensor = model.get_layer('CBAM_block' + block_num).channel.channel_attention_map\n",
    "    \n",
    "    # Tensor postprocessing\n",
    "    attention_resized = attention_tensor[0, 0, 0, :]\n",
    "    attention_map = attention_array.numpy()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e597d-73c6-4c55-838e-d533bc2f8e4a",
   "metadata": {},
   "source": [
    "## Rescale attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0f24bf4c-4a0c-4729-899f-8547741fa5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_up(array, target_size=64):\n",
    "    y = target_size // array.shape[0]\n",
    "    x = target_size // array.shape[1]\n",
    "    \n",
    "    return np.kron(array, np.ones((y, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706146e8-c51f-45f3-884d-53dd5a9394af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense(spatial_attention, technique='avg'):\n",
    "    weight = -1\n",
    "    \n",
    "    if technique == 'avg':\n",
    "        weight = np.mean(spatial_attention)\n",
    "    elif technique == 'max':\n",
    "        weight = np.max(spatial_attention)\n",
    "    elif technique == 'min':\n",
    "        weight = np.min(spatial_attention)\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd812ff-a61d-4464-aa37-e72b1a701cb7",
   "metadata": {},
   "source": [
    "## Comparision using kendell corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "46ff9f6d-434e-4bcd-8bdf-9aa76dbaef7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "The GradCAM explanation is 0.3200 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "The GradCAM explanation is 0.3610 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "16\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "The GradCAM explanation is 0.2472 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "The GradCAM explanation is 0.2625 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "The GradCAM explanation is 0.3008 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "The GradCAM explanation is 0.2496 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "The GradCAM explanation is 0.3009 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "7\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "The GradCAM explanation is 0.2531 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "The GradCAM explanation is 0.3976 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "The GradCAM explanation is 0.2846 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "The GradCAM explanation is 0.3296 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "The GradCAM explanation is 0.2518 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "The GradCAM explanation is 0.3802 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "The GradCAM explanation is 0.3454 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "The GradCAM explanation is 0.2265 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "16\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "The GradCAM explanation is 0.2379 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "16\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "The GradCAM explanation is 0.2247 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "17\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "The GradCAM explanation is 0.1761 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "18\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "The GradCAM explanation is 0.3309 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "The GradCAM explanation is 0.2667 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "The GradCAM explanation is 0.2156 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "21\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "The GradCAM explanation is 0.3154 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "22\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "The GradCAM explanation is 0.2478 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "23\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "The GradCAM explanation is 0.2958 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "24\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "The GradCAM explanation is 0.2669 similar to the attention weights\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "The GradCAM explanation is 0.3297 similar to the attention weights\n"
     ]
    }
   ],
   "source": [
    "similarities = [] # For each letter\n",
    "attention_level = 0\n",
    "\n",
    "for letter in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "    # Dynamically retrieve image arrays (assuming they're defined like image_array_A, etc.)\n",
    "    image_array = globals()[f\"image_array_{letter.lower()}\"]\n",
    "    image_input = globals()[f\"image_input_{letter.lower()}\"]\n",
    "\n",
    "    # Getting Grad-CAM explanation\n",
    "    gradcam_weights = get_gradcam(functional_model, image_input, layer_name)\n",
    "    \n",
    "    # Retrieving and postprocessing the attention weights\n",
    "    if ATTENTION_MODULE == 'CBAM':\n",
    "        attention_channel = get_cbam_channel(model, image_input, attention_level)\n",
    "        attention_spatial = get_cbam_spatial(model, image_input, attention_level)\n",
    "\n",
    "        channel_condensed = condense(attention_channel)\n",
    "        spatial_scaled = scale_up(attention_spatial)\n",
    "\n",
    "        attention_weights = spatial_scaled * channel_condensed\n",
    "        \n",
    "    elif ATTENTION_MODULE == 'SE':\n",
    "        attention_channel = get_se(model, image_input, attention_level)\n",
    "        channel_condensed = condense(attention_channel)\n",
    "\n",
    "        attention_weights = scale_up(channel_condensed)\n",
    "\n",
    "    # Flatten for correlation\n",
    "    kendal, _ = kendalltau(gradcam_weights.flatten(), attention_weights.flatten())\n",
    "    similarities.append(kendal)\n",
    "    print(f\"The GradCAM explanation is {kendal:.4f} similar to the attention weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7da7a553-8362-4394-94a1-11d8100215f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarity: 0.39758221911430036\n",
      "Minimum of similarity: 0.17608298756361607\n",
      "Mean similarity: 0.2853220559057988\n",
      "Standard deviation of similarity: 0.05310430460577593\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum similarity: {np.max(similarities)}')\n",
    "print(f'Minimum of similarity: {np.min(similarities)}')\n",
    "print(f'Mean similarity: {np.mean(similarities)}')\n",
    "print(f'Standard deviation of similarity: {np.std(similarities)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisualAttentionAsExplanation",
   "language": "python",
   "name": "visualattentionasexplanation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
